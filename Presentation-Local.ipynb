{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team : Chaos\n",
    "## Idea : Face Recognition to walk-in into the office"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Abstract : Walk in to the office, without the need of swiping your card. Automatic face recognition based entry and exit system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theme : Colleague Experience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Face recognition problems commonly fall into two categories: \n",
    "\n",
    "- **Face Verification** - \"is this the claimed person?\". For example, at some airports, you can pass through customs by letting a system scan your passport and then verifying that you (the person carrying the passport) are the correct person. A mobile phone that unlocks using your face is also using face verification. This is a 1:1 matching problem. \n",
    "- **Face Recognition** - \"who is this person?\". For example, the video below of some employees entering the office without needing to otherwise identify themselves. This is a 1:K matching problem. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "from predictor import Predict\n",
    "from register import Register"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Known face : vivek\n",
      "Known face : pradeep\n",
      "Known face : anish\n",
      "Known face : asha\n",
      "Known face : kirti\n",
      "Known face : bharath\n",
      "Known face : akshat\n",
      "Known face : vishnu\n"
     ]
    }
   ],
   "source": [
    "register = Register()\n",
    "\n",
    "for img in os.listdir('known_faces'):\n",
    "    name = os.path.splitext(img)[0]\n",
    "    if name == '.ipynb_checkpoints':\n",
    "        continue\n",
    "    print(\"Known face : {}\".format(name))\n",
    "    register.register_user(name, os.path.join('known_faces', img))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_locations = []\n",
    "face_encodings = []\n",
    "face_names = []\n",
    "process_this_frame = True\n",
    "\n",
    "video_capture = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "\n",
    "    ret, frame = video_capture.read()\n",
    "    small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n",
    "    # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\n",
    "    rgb_small_frame = small_frame[:, :, ::-1]\n",
    "\n",
    "    # Only process every other frame of video to save time\n",
    "    if process_this_frame:\n",
    "        face_locations, face_names = Predict.get_predictions(rgb_small_frame, register)\n",
    "\n",
    "    process_this_frame = not process_this_frame\n",
    "\n",
    "    # Display the results\n",
    "    for (top, right, bottom, left), name in zip(face_locations, face_names):\n",
    "        # Scale back up face locations since the frame we detected in was scaled to 1/4 size\n",
    "        top *= 4\n",
    "        right *= 4\n",
    "        bottom *= 4\n",
    "        left *= 4\n",
    "\n",
    "        # Draw a box around the face\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "\n",
    "        # Draw a label with a name below the face\n",
    "        #cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 255, 0), cv2.FILLED)\n",
    "        font = cv2.FONT_HERSHEY_DUPLEX\n",
    "        cv2.putText(frame, name, (left + 6, top - 20), font, 1.0, (0, 0, 255), 1)\n",
    "\n",
    "\n",
    "    # Display the resulting image\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Hit 'q' on the keyboard to quit!\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release handle to the webcam\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "\n",
    "- Florian Schroff, Dmitry Kalenichenko, James Philbin (2015). [FaceNet: A Unified Embedding for Face Recognition and Clustering](https://arxiv.org/pdf/1503.03832.pdf)\n",
    "- Our implementation also took a lot of inspiration from the official FaceNet github repository: https://github.com/davidsandberg/facenet \n",
    "- Implementation of Facenet available as Pip Package (face-recognition) : https://pypi.org/project/face_recognition/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"resources/Motivation.mp4\" width=\"700\" height=\"394\" frameborder=\"0\" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe><p><a href=\"https://vimeo.com/26763844\">BAXTER DURY - CLAIRE (Dir Cut)</a> from <a href=\"https://vimeo.com/dannysangra\">Danny Sangra</a> on <a href=\"https://vimeo.com\">Vimeo</a>.</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe src=\"resources/Motivation.mp4\" width=\"700\" height=\"394\" frameborder=\"0\" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe><p><a href=\"https://vimeo.com/26763844\">BAXTER DURY - CLAIRE (Dir Cut)</a> from <a href=\"https://vimeo.com/dannysangra\">Danny Sangra</a> on <a href=\"https://vimeo.com\">Vimeo</a>.</p>')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Business Value : \n",
    "1. Travelling across offices would be easier. No need to raised request on IIQ and swim through tens of PRC groups for activating the access. \n",
    "2. Reduce walk-in time. Currently, in most of our facilites, colleagues first swipe their RFC card and then do the bio-metric. This process easily take about 5-7 seconds per day.\n",
    "\n",
    "#### Technical Innovation\n",
    "1. To best of our knoweldge, first solution which is build purely on Deep Learning.\n",
    "2. Agnostic from deployment systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
